{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercased, f\"([{string.punctuation}])\", r\" \\1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CsvDatasetV2 element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None),)>\n",
      "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7f107211a7f0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# dataset = pd.read_csv('clickbait_data.csv')\n",
    "dataset = tf.data.experimental.CsvDataset('clickbait_data.csv', header=True, record_defaults=(str(),))\n",
    "print(dataset)\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(dataset)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vectorize_layer)\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "dataset = dataset.map(prepare_lm_inputs_labels)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  15996/Unknown - 223s 14ms/step - loss: 0.7409 - dense_8_loss: 0.7409generated text:\n",
      "this movie is the most hilarious about a damn thing in the year                               \n",
      "\n",
      "15999/15999 [==============================] - 226s 14ms/step - loss: 0.7410 - dense_8_loss: 0.7410\n",
      "Epoch 2/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.6097 - dense_8_loss: 0.6097generated text:\n",
      "this movie is actually real thing you 'll ever take                                  \n",
      "\n",
      "15999/15999 [==============================] - 226s 14ms/step - loss: 0.6097 - dense_8_loss: 0.6097\n",
      "Epoch 3/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.5519 - dense_8_loss: 0.5519generated text:\n",
      "this movie is a real new year in india 's biggest queer women and they are tired day                          \n",
      "\n",
      "15999/15999 [==============================] - 223s 14ms/step - loss: 0.5519 - dense_8_loss: 0.5519\n",
      "Epoch 4/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.5046 - dense_8_loss: 0.5046generated text:\n",
      "this movie is the most canadian election                                     \n",
      "\n",
      "15999/15999 [==============================] - 225s 14ms/step - loss: 0.5046 - dense_8_loss: 0.5046\n",
      "Epoch 5/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.4627 - dense_8_loss: 0.4627generated text:\n",
      "this movie is the most powerful photos of this week                                  \n",
      "\n",
      "15999/15999 [==============================] - 222s 14ms/step - loss: 0.4627 - dense_8_loss: 0.4627\n",
      "Epoch 6/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.4279 - dense_8_loss: 0.4279generated text:\n",
      "this movie is going a hilarious real mom for the week of september                               \n",
      "\n",
      "15999/15999 [==============================] - 221s 14ms/step - loss: 0.4279 - dense_8_loss: 0.4279\n",
      "Epoch 7/25\n",
      "15998/15999 [============================>.] - ETA: 0s - loss: 0.4007 - dense_8_loss: 0.4007generated text:\n",
      "this movie is a true real thing you 're the right                                 \n",
      "\n",
      "15999/15999 [==============================] - 222s 14ms/step - loss: 0.4007 - dense_8_loss: 0.4007\n",
      "Epoch 8/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.3794 - dense_8_loss: 0.3794generated text:\n",
      "this movie is set of the art history by election history , jays : canada                             \n",
      "\n",
      "15999/15999 [==============================] - 224s 14ms/step - loss: 0.3794 - dense_8_loss: 0.3794\n",
      "Epoch 9/25\n",
      "15998/15999 [============================>.] - ETA: 0s - loss: 0.3639 - dense_8_loss: 0.3639generated text:\n",
      "this movie is the year of see its most 100 years of bollywood stars story                             \n",
      "\n",
      "15999/15999 [==============================] - 221s 14ms/step - loss: 0.3639 - dense_8_loss: 0.3639\n",
      "Epoch 10/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.3493 - dense_8_loss: 0.3493generated text:\n",
      "this movie is going to have a dad you seen                                  \n",
      "\n",
      "15999/15999 [==============================] - 222s 14ms/step - loss: 0.3493 - dense_8_loss: 0.3493\n",
      "Epoch 11/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.3381 - dense_8_loss: 0.3381generated text:\n",
      "this movie is the election , jays overwhelmed and winning at winning the baftas                              \n",
      "\n",
      "15999/15999 [==============================] - 223s 14ms/step - loss: 0.3381 - dense_8_loss: 0.3381\n",
      "Epoch 12/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.3288 - dense_8_loss: 0.3288generated text:\n",
      "this movie is to see how the most flawless photos of this person                               \n",
      "\n",
      "15999/15999 [==============================] - 220s 14ms/step - loss: 0.3288 - dense_8_loss: 0.3288\n",
      "Epoch 13/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.3197 - dense_8_loss: 0.3197generated text:\n",
      "this movie is the most important of the last of 2015                                 \n",
      "\n",
      "15999/15999 [==============================] - 222s 14ms/step - loss: 0.3197 - dense_8_loss: 0.3197\n",
      "Epoch 14/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.3139 - dense_8_loss: 0.3139generated text:\n",
      "this movie is a hilarious talking about the period                                   \n",
      "\n",
      "15999/15999 [==============================] - 222s 14ms/step - loss: 0.3139 - dense_8_loss: 0.3139\n",
      "Epoch 15/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.3063 - dense_8_loss: 0.3063generated text:\n",
      "this movie is going a bed for a night last and what is very it                             \n",
      "\n",
      "15999/15999 [==============================] - 234s 15ms/step - loss: 0.3063 - dense_8_loss: 0.3063\n",
      "Epoch 16/25\n",
      "15998/15999 [============================>.] - ETA: 0s - loss: 0.3008 - dense_8_loss: 0.3008generated text:\n",
      "this movie is posters significantly improved by amazon reviews                                   \n",
      "\n",
      "15999/15999 [==============================] - 238s 15ms/step - loss: 0.3008 - dense_8_loss: 0.3008\n",
      "Epoch 17/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.2949 - dense_8_loss: 0.2949generated text:\n",
      "this movie is amazing netflix is talking about the new time                                 \n",
      "\n",
      "15999/15999 [==============================] - 240s 15ms/step - loss: 0.2949 - dense_8_loss: 0.2949\n",
      "Epoch 18/25\n",
      "15998/15999 [============================>.] - ETA: 0s - loss: 0.2896 - dense_8_loss: 0.2896generated text:\n",
      "this movie is a tragic love story based on your zodiac sign                                \n",
      "\n",
      "15999/15999 [==============================] - 243s 15ms/step - loss: 0.2896 - dense_8_loss: 0.2896\n",
      "Epoch 19/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.2852 - dense_8_loss: 0.2852generated text:\n",
      "this movie is so going to brighten your day                                   \n",
      "\n",
      "15999/15999 [==============================] - 243s 15ms/step - loss: 0.2852 - dense_8_loss: 0.2852\n",
      "Epoch 20/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.2802 - dense_8_loss: 0.2802generated text:\n",
      "this movie is a real life of thanksgiving                                    \n",
      "\n",
      "15999/15999 [==============================] - 243s 15ms/step - loss: 0.2802 - dense_8_loss: 0.2802\n",
      "Epoch 21/25\n",
      "15999/15999 [==============================] - ETA: 0s - loss: 0.2764 - dense_8_loss: 0.2764generated text:\n",
      "this movie is a rumor that drake drake died drake died got started                               \n",
      "\n",
      "15999/15999 [==============================] - 242s 15ms/step - loss: 0.2764 - dense_8_loss: 0.2764\n",
      "Epoch 22/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.2729 - dense_8_loss: 0.2729generated text:\n",
      "this movie is a tragic love for in pictures of the new pictures that 'll all \"the last time                         \n",
      "\n",
      "15999/15999 [==============================] - 242s 15ms/step - loss: 0.2729 - dense_8_loss: 0.2729\n",
      "Epoch 23/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.2689 - dense_8_loss: 0.2689generated text:\n",
      "this movie is how to watch actually the golden globe nominations                                 \n",
      "\n",
      "15999/15999 [==============================] - 243s 15ms/step - loss: 0.2689 - dense_8_loss: 0.2689\n",
      "Epoch 24/25\n",
      "15996/15999 [============================>.] - ETA: 0s - loss: 0.2645 - dense_8_loss: 0.2645generated text:\n",
      "this movie is a what would make your kid question better in life                               \n",
      "\n",
      "15999/15999 [==============================] - 245s 15ms/step - loss: 0.2645 - dense_8_loss: 0.2645\n",
      "Epoch 25/25\n",
      "15997/15999 [============================>.] - ETA: 0s - loss: 0.2628 - dense_8_loss: 0.2628generated text:\n",
      "this movie is a rumor that started therapy a treating women                                 \n",
      "\n",
      "15999/15999 [==============================] - 243s 15ms/step - loss: 0.2628 - dense_8_loss: 0.2628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fec3a8940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(dataset, verbose=1, epochs=25, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 07:47:02.664499: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, multi_head_attention_2_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mini_gpt.pth/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mini_gpt.pth/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(filepath='mini_gpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most powerful photos show of this week                                                                                                                                                                                                    powerful photos show of this week                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "def sample_from(logits):\n",
    "    logits, indices = tf.math.top_k(logits, k=10, sorted=True)\n",
    "    indices = np.asarray(indices).astype(\"int32\")\n",
    "    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "    preds = np.asarray(preds).astype(\"float32\")\n",
    "    return np.random.choice(indices, p=preds)\n",
    "\n",
    "\n",
    "def complete_clickbait_sentence(sentence_start: str):\n",
    "    start_prompt = sentence_start\n",
    "    start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "\n",
    "    start_tokens = [_ for _ in start_tokens]\n",
    "    num_tokens_generated = 0\n",
    "    tokens_generated = []\n",
    "    while num_tokens_generated <= 200:\n",
    "        pad_len = maxlen - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = np.array([x])\n",
    "        y, _ = model.predict(x)\n",
    "        sample_token = sample_from(y[0][sample_index])\n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "    txt = \" \".join(\n",
    "        [vocab[_] for _ in start_tokens + tokens_generated]\n",
    "    )\n",
    "    return txt\n",
    "\n",
    "print(complete_clickbait_sentence(\"the most\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09971b477cd1c2ccd54fcefe77bcc2c3e5de4dd8a783f2d252d77180e9bb9c4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
